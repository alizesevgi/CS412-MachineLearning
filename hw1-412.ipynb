{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw1_alizesevgi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voHKcAfRfdNY"
      },
      "source": [
        "# **CS412 - Machine Learning - 2021 Summer**\n",
        "## **Homework 1**\n",
        "100 pts\n",
        "\n",
        "\n",
        "## **Goal**\n",
        "\n",
        "The goal of this homework is three-fold:\n",
        "\n",
        "*   Introduction to the machine learning experimental set up \n",
        "*   Gain experience with Decision tree approach\n",
        "*   Gain experience with the Scikit library\n",
        "\n",
        "## **Dataset**\n",
        "This dataset is taken from the following Kaggle link and simplified for the Homework 1: https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv\n",
        "\n",
        "**Download the data from Sucourse.**\n",
        "You must use 20% of the data for validation and 20% for test: **Training: 60%, Validation: 20%, Test: 20%**\n",
        "\n",
        "## **Task**\n",
        "Build a decision tree classifier with the scikit library function calls to predict whether a data scientist candidate is going to look for a new job or will work for the company - **target column** is the target variable.\n",
        "\n",
        "**You should check the documentation \"cs412_hw1_desc\" to understand the task wholly.**\n",
        "\n",
        "## **Software: You may find the necessary function references here:**\n",
        "http://scikit-learn.org/stable/supervised_learning.html\n",
        "\n",
        "## **Submission:**\n",
        "Fill this notebook and submit this document with a link to #your Colab notebook \n",
        "(make sure to include the link obtained from the #share link on top right)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YOYiWvHbNDW"
      },
      "source": [
        "## 0) Initialize\n",
        "\n",
        "*   First make a copy of the notebook given to you as a starter.\n",
        "\n",
        "*   Make sure you choose Connect form upper right.\n",
        "\n",
        "*   You may upload the data to the section on your left on Colab, than right click on the .csv file and get the path of the file by clicking on \"Copy Path\". You will be using it when loading the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pxt5hs72KVM"
      },
      "source": [
        "## 1) Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf5uweT82KVN"
      },
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os.path import join\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM-wwHR8qL0M"
      },
      "source": [
        "## 2) Load training dataset\n",
        "\n",
        "*  Read the .csv file with pandas library\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz3iMpjVfa5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d71a8a7-74f0-4a0f-933c-0db4f96bf582"
      },
      "source": [
        "# Read data\n",
        "from google.colab import drive\n",
        "drive.mount(\"./drive\")\n",
        "\n",
        "path_prefix = \"./drive/My Drive\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "6C747AHoKjxg",
        "outputId": "2000e62f-41fd-493e-f007-961c936dda51"
      },
      "source": [
        "fname = \"data_scientist_job_change.csv\"\n",
        "df = pd.read_csv(join(path_prefix, fname))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city_development_index</th>\n",
              "      <th>relevent_experience</th>\n",
              "      <th>education_level</th>\n",
              "      <th>experience</th>\n",
              "      <th>training_hours</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.920</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>36</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.776</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>47</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.624</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>83</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.789</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.767</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   city_development_index  relevent_experience  ...  training_hours  target\n",
              "0                   0.920                    1  ...              36     1.0\n",
              "1                   0.776                    0  ...              47     0.0\n",
              "2                   0.624                    0  ...              83     0.0\n",
              "3                   0.789                    0  ...              52     1.0\n",
              "4                   0.767                    1  ...               8     0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NdW2ItjHLxJ"
      },
      "source": [
        "## 3) Understanding the dataset\n",
        "\n",
        "There are alot of functions that can be used to know more about this dataset\n",
        "\n",
        "- What is the shape of the training set (num of samples X number of attributes) ***[shape function can be used]***\n",
        "\n",
        "- Display attribute names ***[columns function can be used]***\n",
        "\n",
        "- Display the first 5 rows from training dataset ***[head or sample functions can be used]***\n",
        "\n",
        "- Display number of nan values on each column ***[.isna() method can be used]***\n",
        "\n",
        "Note: Understanding the features, possibly removing some features etc. is an important part in building an ML system, but for this homework this is not really necessary as the features are already transformed and simplified.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA_AjGQasjvS",
        "outputId": "903199c3-7e7c-4a94-8387-1f202be4c202"
      },
      "source": [
        "# print shape\n",
        "print('Data Dimensionality: ',df.shape )\n",
        "\n",
        "\n",
        "\n",
        "# print first 5 rows in your dataset\n",
        "print('Head of Data: ', df.columns)\n",
        "\n",
        "\n",
        "\n",
        "# print nan values for each column\n",
        "print('NaN values: ',df.isna().sum() )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Dimensionality:  (19158, 6)\n",
            "Head of Data:  Index(['city_development_index', 'relevent_experience', 'education_level',\n",
            "       'experience', 'training_hours', 'target'],\n",
            "      dtype='object')\n",
            "NaN values:  city_development_index      0\n",
            "relevent_experience         0\n",
            "education_level           460\n",
            "experience                 65\n",
            "training_hours              0\n",
            "target                      0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oJGgdSV2KVP"
      },
      "source": [
        "## 4) Handling Missing Data\n",
        "Simply drop the rows with NaN values or fill the NaN values with mode, median or mean value of the column. .dropna() method can be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv1TqiiY2KVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1761967-fba6-4c6b-a0e6-5ccf4775584d"
      },
      "source": [
        "###\n",
        "#print(df.mean())\n",
        "#df['education_level'] = df['education_level'].fillna((df['education_level'].mean()))\n",
        "#df['experience'] = df['experience'].fillna((df['experience'].mean()))\n",
        "#print(df.isna().sum())\n",
        "\n",
        "print(df.isna().sum())\n",
        "\n",
        "df.dropna(subset=[\"education_level\"], inplace = True)\n",
        "df = df.reset_index(drop=True)\n",
        "print(\"Missing values in column: \",df.education_level.isna().sum())\n",
        "\n",
        "df.dropna(subset=[\"experience\"], inplace = True)\n",
        "df = df.reset_index(drop=True)\n",
        "print(\"Missing values in column: \",df.experience.isna().sum())\n",
        "print(df.isna().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "city_development_index      0\n",
            "relevent_experience         0\n",
            "education_level           460\n",
            "experience                 65\n",
            "training_hours              0\n",
            "target                      0\n",
            "dtype: int64\n",
            "Missing values in column:  0\n",
            "Missing values in column:  0\n",
            "city_development_index    0\n",
            "relevent_experience       0\n",
            "education_level           0\n",
            "experience                0\n",
            "training_hours            0\n",
            "target                    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vop4rwZVxh9Z"
      },
      "source": [
        "## 5) Shuffle and Split training, test and validation sets as 60%-20%-20%, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEhk8R24xhdY"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Shuffle the training data and define X and y\n",
        "\n",
        "df.head()\n",
        "feature_cols = ['city_development_index', 'relevent_experience', 'education_level', 'experience', 'training_hours']\n",
        "X = df[feature_cols]\n",
        "y= df.target\n",
        "\n",
        "# Split as 60%-20%-20%\n",
        "\n",
        "X_trainx, X_test, y_trainx, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trainx, y_trainx, test_size=0.5, random_state=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR1oMsPu0AV_"
      },
      "source": [
        "## 6) Train a decision tree classifier on development/train data and do model selection using the validation data\n",
        "\n",
        "* Train 5 decision tree classifiers with different values of \"min_samples_split\" which is the minimum number of samples required to split an internal node:  min_samples_split = [2, 4, 6, 8, 10]. \n",
        "* Evaluate the 5 models on validation set and choose the best one.\n",
        "* Plot the train and validation set errors for those 5 settings - on one plot. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv6oac-T3Wy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5213891d-e237-4467-82cc-b509747615bc"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Train decision tree classifiers & Evaluate on validation set\n",
        "#I will use k-fold validation\n",
        "clf_1 = DecisionTreeClassifier()\n",
        "clf_2 = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 4)\n",
        "clf_3 = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 6)\n",
        "clf_4 = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 8)\n",
        "clf_5 = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 10)\n",
        "\n",
        "clf_1 = clf_1.fit(X_train,y_train)\n",
        "y_pred1 = clf_1.predict(X_val)\n",
        "print(\"Accuracy1:\",metrics.accuracy_score(y_val, y_pred1))\n",
        "\n",
        "clf_2 = clf_2.fit(X_train,y_train)\n",
        "y_pred2 = clf_2.predict(X_val)\n",
        "print(\"Accuracy2:\",metrics.accuracy_score(y_val, y_pred2))\n",
        "\n",
        "clf_3 = clf_3.fit(X_train,y_train)\n",
        "y_pred3 = clf_3.predict(X_val)\n",
        "print(\"Accuracy3:\",metrics.accuracy_score(y_val, y_pred3))\n",
        "\n",
        "clf_4 = clf_4.fit(X_train,y_train)\n",
        "y_pred4 = clf_4.predict(X_val)\n",
        "print(\"Accuracy4:\",metrics.accuracy_score(y_val, y_pred4))\n",
        "\n",
        "clf_5 = clf_5.fit(X_train,y_train)\n",
        "y_pred5 = clf_5.predict(X_val)\n",
        "print(\"Accuracy5:\",metrics.accuracy_score(y_val, y_pred5))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy1: 0.6928303236188093\n",
            "Accuracy2: 0.7021276595744681\n",
            "Accuracy3: 0.7126765599856965\n",
            "Accuracy4: 0.7139281244412659\n",
            "Accuracy5: 0.7205435365635616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a86e14y0885K",
        "outputId": "cdfdceab-31cf-4e5f-dfe5-53207a72b7bd"
      },
      "source": [
        "# Plot errors\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_val, y_pred1))\n",
        "print(classification_report(y_val, y_pred1))\n",
        "\n",
        "print(confusion_matrix(y_val, y_pred2))\n",
        "print(classification_report(y_val, y_pred2))\n",
        "\n",
        "print(confusion_matrix(y_val, y_pred3))\n",
        "print(classification_report(y_val, y_pred3))\n",
        "\n",
        "print(confusion_matrix(y_val, y_pred4))\n",
        "print(classification_report(y_val, y_pred4))\n",
        "\n",
        "print(confusion_matrix(y_val, y_pred5))\n",
        "print(classification_report(y_val, y_pred5))\n",
        "#clearly pred5 has better precision\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3337  857]\n",
            " [ 861  538]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.80      0.80      4194\n",
            "         1.0       0.39      0.38      0.39      1399\n",
            "\n",
            "    accuracy                           0.69      5593\n",
            "   macro avg       0.59      0.59      0.59      5593\n",
            "weighted avg       0.69      0.69      0.69      5593\n",
            "\n",
            "[[3395  799]\n",
            " [ 867  532]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.81      0.80      4194\n",
            "         1.0       0.40      0.38      0.39      1399\n",
            "\n",
            "    accuracy                           0.70      5593\n",
            "   macro avg       0.60      0.59      0.60      5593\n",
            "weighted avg       0.70      0.70      0.70      5593\n",
            "\n",
            "[[3454  740]\n",
            " [ 867  532]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.82      0.81      4194\n",
            "         1.0       0.42      0.38      0.40      1399\n",
            "\n",
            "    accuracy                           0.71      5593\n",
            "   macro avg       0.61      0.60      0.60      5593\n",
            "weighted avg       0.70      0.71      0.71      5593\n",
            "\n",
            "[[3442  752]\n",
            " [ 848  551]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.82      0.81      4194\n",
            "         1.0       0.42      0.39      0.41      1399\n",
            "\n",
            "    accuracy                           0.71      5593\n",
            "   macro avg       0.61      0.61      0.61      5593\n",
            "weighted avg       0.71      0.71      0.71      5593\n",
            "\n",
            "[[3482  712]\n",
            " [ 851  548]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.83      0.82      4194\n",
            "         1.0       0.43      0.39      0.41      1399\n",
            "\n",
            "    accuracy                           0.72      5593\n",
            "   macro avg       0.62      0.61      0.61      5593\n",
            "weighted avg       0.71      0.72      0.72      5593\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boqe46St1--f"
      },
      "source": [
        "## 7) Test your CHOSEN classifier on Test set\n",
        "\n",
        "- Apply same pre-processing as training data (probably none)\n",
        "- Predict the labels of testing data **using the best chosen SINGLE model out of the models that you have tried from step 6 (you have selected your model according to your validation results)** and report the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPLke8jyFGng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1c47d6-45a3-44b2-8b3c-edc23f9ad68e"
      },
      "source": [
        "# You may want to train your BEST decision tree model on both training and validation sets. To merge these two, you may use\n",
        "# concat() method of pandas\n",
        "#i dont have to merge them, i created X_trainx and y_trainx before splitting it into test and validation\n",
        "\n",
        "#i will use min_samples_split 10 bc overall, that one has given the best accuracy, with only 8 as a competition rarely.\n",
        "\n",
        "# test prediction using a decision tree with all default parameters and ..... min-split value\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 10)\n",
        "clf = clf.fit(X_trainx,y_trainx)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Report your accuracy\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7146688120139447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG3473I9dGE8"
      },
      "source": [
        "## 8) Notebook & Report \n",
        "\n",
        "**Notebook: We may just look at your notebook results; so make sure each cell is run and  outputs are on place.**\n",
        "\n",
        "**Report: Write an at most 1/2 page summary of your approach to this problem at the end of your notebook**; this should be like an abstract of a paper or the executive summary (you aim for clarity and passing on information, not going to details about known facts such as what dec. trees or dataset details are, assuming they are known to people in your research area). \n",
        "\n",
        "**Must include statements such as:**\n",
        "\n",
        "- **20pts** - The problem definition in 1-2 lines and explanation of the features (which is an ordinal variable? which is binary? etc.)\n",
        "\n",
        "- **20pts** - What type of model is Decision Tree? (Unsupervised or supervised - explanation? Classification or regression - explanation?)\n",
        "\n",
        "- **20pts** - Why do we have a seperate validation set?\n",
        " \n",
        "- **20pts** - Give the validation accuracies for different hyperparameters **in a table** and state which one you selected\n",
        " \n",
        "- **20pts** - State what your test results are with the chosen method, parameters: e.g. \"We have obtained the best results with the min sample split = .... , giving classification accuracy of …% on test data….\"\"\n",
        "\n",
        "The **last day for the submissions** is Tuesday, 19 July 2021, 12:00 AM. **Late submissions** are accepted until Thursday 21 July 2021, 12:00 AM with a **-10 pts penalty**.\n",
        "\n",
        "*You will get full points from here as long as you have a good (enough) summary of your work, regardless of your best performance or what you have decided to talk about in the last few lines.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxu_Vmdvobj2"
      },
      "source": [
        "Q1: We are using city_development_index (real_valued),\trelevent_experience (binary),\teducation_level (ordinal),\texperience (real_valued) and\ttraining_hours (real_valued) to determine if the target will be 0 or 1 (binary).  \n",
        "\n",
        "Q2: It is classification because we are just looking to see if they should be hired or not. So we are looking to classify them. It is supervised, because this data is labeled and this model learns from the labeled data and makes predictions for new examples.\n",
        "\n",
        "Q3: We can't use the test or train sets, because the model knows the target for train sets already and it's trying to predict the test set's target. And also, we want to validate our model because we are trying to tune the decision tree and get a better accuracy. \n",
        "\n",
        "Q4: \n",
        "**min_sample_split......accuracy**\n",
        "\n",
        "2...........................................0.69\n",
        "\n",
        "4...........................................0.70\n",
        "\n",
        "6...........................................0.712\n",
        "\n",
        "8...........................................0.713\n",
        "\n",
        "***10.......................................0.72***\n",
        "\n",
        "\n",
        "\n",
        "Q5: We have obtained the best results with the min sample split = 10, giving classification accuracy of %72 on test data."
      ]
    }
  ]
}